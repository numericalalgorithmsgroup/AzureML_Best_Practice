<p><a href="https://www.nag.com"><img src="https://www.nag.com/themes/custom/nag/logo.png" alt="NAG Logo" /></a></p>
<h4 id="author-phil-tooley---phil.tooleynag.co.uk">Author: Phil Tooley - <a href="mailto:phil.tooley@nag.co.uk">phil.tooley@nag.co.uk</a></h4>
<h1 id="tutorial-beeond-azureml-a-high-performance-filesystem-for-hpc-scale-ml">Tutorial: BeeOND + AzureML: A High Performance Filesystem for HPC-scale ML</h1>
<p>The explosive growth in both the power and application of deep learning in recent years has been fuelled both by advances in GPU computing hardware and also by the ever growing quantities of data available in an increasingly connected world. As a result, modern high-performance machine learning workloads can have huge datasets, well beyond 1TB in size, and scale to run on clusters with tens or even hundreds of GPUs. However, the massive compute power of modern GPUs comes with similarly massive data throughput requirements, with a single GPU performing deep learning training capable of consuming data at a rate of hundreds of MB/s and training clusters at rates of 10s to 100s of GB/s. Avoiding bottlenecks due to data starvation requires a high performance data storage solution which can scale to meet the capacity and throughput demands of the latest deep learning models and GPU hardware.</p>
<p>The Azure Machine Learning platform provides a number of options for scalable dataset storage, including directly mounting both Azure Blob and Azure Files. These services offer flexibility and huge storage space, but what if your workload has I/O requirements that exceed the capabilities of these network-attached storage options? Caching is a viable strategy for datasets which can fit entirely in local machine memory or local disk storage, but for extreme sized datasets this is not an option and we are restricted to streaming data as needed from some form of highly scalable network storage. In this tutorial we will demonstrate using the BeeOND filesystem as a cost-effective and scalable solution for AzureML workloads with extreme I/O needs. Our <a href="#testing-the-performance">performance benchmarks</a> show that a cluster of 4 nodes can achieve over 15GB/s total system bandwidth for both read and write.</p>
<h2 id="workloads-with-extreme-io-requirements-cosmoflow">Workloads with Extreme I/O Requirements: CosmoFlow</h2>
<p>As an example of a ML workload with extreme I/O requirements, let’s look at CosmoFlow - a CNN-based model for learning cosmological parameters of the universe from large 3D maps of dark matter distributions. CosmoFlow is one of the <a href="https://mlcommons.org/en/news/mlperf-hpc-v07/">MLPerf HPC benchmark</a> workloads, chosen because of its extreme computational requirements: it has a massive 5.5TB benchmark dataset and distributed training scales efficiently to 512 NVIDIA V100 GPUs and beyond, each GPU consuming data at an average rate of ~300MB/s. As is typical for ML workloads, CosmoFlow iterates over the full dataset many times in a single run, however the dataset is far too large to be cached in memory or node-local storage. As a result we will need to stream the data from some kind of high performance data store.</p>
<p>For this tutorial we used the ND40rs v2 instance type in a 4 node cluster for a total of 32 GPUs - this is a fairly modest cluster for the problem size but is already large enough to demonstrate the problem of meeting I/O bandwidth requirements. Each cluster node has 8 GPUs giving a per-node throughput requirement of 2.5GB/s or 10GB/s for the whole cluster to train CosmoFlow without an I/O bottleneck. Given that Azure storage accounts have a maximum egress bandwidth of roughly 6.25GB/s (50Gib/s) it is clear that a single storage account will not provide the required performance. Azure NetApp Files is another option for high performance network-attached storage, however maximum bandwidth for a single instance is around 4.5GB/s, so again a single NetApp Files instance will not meet our requirements. We could try using techniques such as sharding across multiple instances of network-attached storage to scale the overall performance, but this approach potentially adds significant additional cost, and fails to address another potential problem: ethernet performance.</p>
<p>Standard ethernet adapters on Azure Virtual Machines are virtualised by the hypervisor. This means that performance enhancing features such as hardware offloading are typically not available to the guest virtual machine. This can limit overall performance, for example when we benchmarked data download performance from Azure Blob storage to a ND40rs v2 instance we found that throughput was limited to around 750MB/s (6Gib/s) because of this issue. Some Azure virtual machine types offer SR-IOV enabled ethernet (known as “accelerated networking”) which gives increased performance using PCI passthrough, but this feature is currently in preview for Azure ML and is not yet recommended for general usage.</p>
<p>Coming back to our ND40rs v2 VMs, however, they each come with around 3TB of local NVMe SSD storage with read performance well in excess of 3GB/s.<sup><a href="#nvme_note">1</a></sup> They also have 200GB/s infiniband cards directly passed through to the guest to provide full-rated performance, so what if we just created a shared high performance filesystem (HPFS) on the fly with these same VMs we are already using for compute?</p>
<p><a name="nvme_note"><sup>1</sup><a/> <em>Of course if the dataset is small enough we can cache a complete copy on each node, but we are looking for a solution that scales to extreme dataset sizes.</em></p>
<h2 id="a-hpfs-on-demand">A HPFS On-Demand</h2>
<p>As it turns out we are far from the first to have had this idea and the developers of the <a href="https://www.beegfs.io">BeeGFS</a> high performance filesystem have produced the <a href="https://www.beegfs.io/wiki/BeeOND">BeeOND</a> (BeeGFS on Demand) tool to do exactly this. There is no reason that other high performance filesystems couldn’t be used in the same way, but the fact that BeeOND is specifically designed for on the fly use makes it the natural choice for this tutorial.</p>
<p>Thanks to the work done by the BeeOND developers, deploying a BeeOND filesystem is a fairly straightforward process, and requires three steps:</p>
<ol type="1">
<li><p>Install the BeeOND software components on all nodes in the cluster</p></li>
<li><p>Enable (passwordless) SSH for the BeeOND tool to communicate between nodes</p></li>
<li><p>Run the BeeOND setup tool on any one of the cluster nodes, specifying which nodes should be part of the filesystem, the local storage to use and where to mount the resulting BeeGFS filesystem. The tool will then connect to and automatically configure all the nodes to set up the filesystem.</p></li>
</ol>
<h2 id="integrating-with-azureml">Integrating with AzureML</h2>
<p>So now we know we can easily create a HPFS on the fly, how do we make it work with Azure ML? In our <a href="https://www.nag.com/blog/tutorial-training-scale-azureml">previous tutorial</a> demonstrating Mask RCNN we showed how to use the Python SDK to configure the full training workflow. This involved creating a compute cluster using the <code>AmlCompute</code> object, configuring a <code>Workspace</code> object to use a custom Docker container for training, and finally configuring and submitting a <code>Run</code> object to launch a training run.</p>
<p>If we want to integrate a BeeOND filesystem into our Azure Ml workflow there are three main questions that we need to answer:</p>
<ol type="1">
<li><p>How do we access an AmlCompute cluster to configure BeeOND?</p></li>
<li><p>How do we configure things so our workload can access our new filesystem?</p></li>
<li><p>How do we efficiently stage our training data to our new filesystem?</p></li>
</ol>
<p>In the rest of this tutorial we will show the steps needed to set up BeeOND and run a simple Benchmarking script via AzureML. In the <a href="#">next tutorial</a> we will show how to fully integrate a BeeOND filesystem into an HPC-scale ML workflow training the CosmoFlow model.</p>
<h2 id="installing-beeond-on-an-amlcompute-cluster">Installing BeeOND on an AmlCompute Cluster</h2>
<p>AzureML can be configured to allow SSH access to our AmlCompute clusters while they are running, in order to perform admin tasks. To enable this we just need to provide an admin username and ssh key when we create the cluster. This can be done with either the ML Studio or the SDK interface. We used the Python SDK in the <a href="https://www.nag.com/blog/tutorial-training-scale-azureml">previous tutorial</a> demonstrating how to set up a single workflow, and we will build on those examples here as we add the BeeOND filesystem. As before, when we demonstrated training Mask RCNN on AzureML, we will begin by creating a compute cluster. This time, however, we must make sure we provide the admin username and ssh public key that we will use to login to the the cluster later. We also need to disable autoscaling of the cluster this time - this prevents us from losing our BeeOND filesystem because a node got shut down for being idle. To do this we set the <code>min_nodes</code> and <code>max_nodes</code> to the same value e.g 4 to ensure the cluster size is fixed and it won’t autoscale.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> azureml.core <span class="im">import</span> Workspace</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> azureml.core.compute <span class="im">import</span> AmlCompute</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>workspace <span class="op">=</span> Workspace.get(<span class="st">&quot;AzureMLDemo&quot;</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>cluster_config <span class="op">=</span> AmlCompute.provisioning_configuration(</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>  vm_size<span class="op">=</span><span class="st">&quot;Standard_ND40rs_v2&quot;</span>,</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>  min_nodes<span class="op">=</span><span class="dv">4</span>,</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>  max_nodes<span class="op">=</span><span class="dv">4</span>,</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>  idle_seconds_before_scaledown<span class="op">=</span><span class="dv">300</span>,</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>  admin_username<span class="op">=</span><span class="st">&quot;clusteradmin&quot;</span>,</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>  admin_user_ssh_key<span class="op">=</span>sshpubkey, <span class="co"># Contents of a public key file</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>  remote_login_port_public_access<span class="op">=</span><span class="st">&quot;Enabled&quot;</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>cluster <span class="op">=</span> AmlCompute.create(workspace, <span class="st">&quot;MyCluster&quot;</span>, cluster_config)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>cluster.wait_for_completion()</span></code></pre></div>
<p>Once the cluster nodes are up and running we can connect to them via SSH and configure BeeOND.</p>
<h3 id="sshing-to-amlcompute-nodes">SSHing to AmlCompute Nodes</h3>
<p>To connect to the nodes in the cluster we need to query the AzureML service for the IP address and ports needed to connect. This information can be found in the ML Studio but since we want to configure multiple nodes it makes sense to use Python scripting to automate the process. To get connection information for all cluster nodes we can use the <code>AmlCompute.list_nodes()</code> method. Once we have the connection information we can then ssh to them manually or use a Python SSH client such as <code>paramiko</code> or <code>parallel-ssh</code> to connect to the nodes and configure them:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>nodelist <span class="op">=</span> cluster.list_nodes()</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Node connection information:&quot;</span>)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, node <span class="kw">in</span> <span class="bu">enumerate</span>(nodelist):</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="st">&quot;</span><span class="ch">\t</span><span class="st">Node </span><span class="sc">{}</span><span class="st">: IP: </span><span class="sc">{}</span><span class="st"> - Port: </span><span class="sc">{}</span><span class="st">&quot;</span>.<span class="bu">format</span>(i, node[<span class="st">&#39;publicIpAddress&#39;</span>], node[<span class="st">&#39;port&#39;</span>]))</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Parallel-SSH connection example:</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pssh.clients <span class="im">import</span> SSHClient</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>client <span class="op">=</span> SSHClient(node[<span class="st">&#39;publicIpAddress&#39;</span>], port<span class="op">=</span>node[<span class="st">&#39;port&#39;</span>], user<span class="op">=</span><span class="st">&quot;clusteradmin&quot;</span>)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>host_out <span class="op">=</span> client.run_command(<span class="st">&#39;nvidia-smi&#39;</span>)</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> line <span class="kw">in</span> host_out.stdout:</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(line)</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>    exit_code <span class="op">=</span> host_out.exit_code</span></code></pre></div>
<p>In the examples provided with this tutorial we have included a <code>ClusterConnector</code> class in the file <a href="https://github.com/numericalalgorithmsgroup/AzureML_Best_Practice/blob/master/beeond/clusterconnector.py"><code>clusterconnector.py</code></a>. This includes all the functionality to create a cluster and manage parallel ssh connections, with methods to copy files and run commands on all nodes at once, or to run them only on the master node.</p>
<p>To use the clusterconnector class to create a new cluster and run a command on all the nodes you would do the following:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> clusterconnector <span class="im">import</span> ClusterConnector</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>clusterconn <span class="op">=</span> ClusterConnector(</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    workspace<span class="op">=</span>workspace,</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    cluster_name<span class="op">=</span><span class="st">&quot;MyCluster&quot;</span>,</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    ssh_key<span class="op">=</span>sshpubkey,</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    vm_type<span class="op">=</span><span class="st">&quot;Standard_ND40rs_v2&quot;</span>,</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    admin_user<span class="op">=</span><span class="st">&quot;clusteradmin&quot;</span>,</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>clusterconn.initialise(min_nodes<span class="op">=</span><span class="dv">4</span>, max_nodes<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>clusterconn.run_on_all_nodes(<span class="st">&#39;nvidia-smi&#39;</span>)</span></code></pre></div>
<p>Now files can be copied to and commands run on all or just the master node using the <code>copy_to_all_nodes</code> and <code>run_on_all_nodes</code> or <code>copy_to_master_node</code> and <code>run_on_master_node</code> methods respectively.</p>
<h3 id="setting-up-ssh-keys">Setting up SSH Keys</h3>
<p>To ensure that our nodes can connect to each other for the BeeOND setup we need to set up passwordless SSH, and also make a nodelist of all the private IP addresses for the nodes in the cluster. In this instance because we have a private cluster we will use passwordless SSH keys, though this may not be appropriate for some production environments. The script below will create a list of all the node private IP addresses, along with a passwordless SSH key pair and copy these files to all the nodes.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>node_list <span class="op">=</span> clusterconn.cluster.list_nodes()</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">&#39;nodefile&#39;</span>, <span class="st">&#39;wt&#39;</span>) <span class="im">as</span> fh:</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> node <span class="kw">in</span> node_list:</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    fh.write(<span class="st">&quot;</span><span class="sc">{}</span><span class="ch">\n</span><span class="st">&quot;</span>.<span class="bu">format</span>(node[<span class="st">&#39;privateIpAddress&#39;</span>]))</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>clusterconn.copy_to_all_nodes(<span class="st">&#39;nodefile&#39;</span>, <span class="st">&#39;~/nodefile&#39;</span>)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>ssh_keygen <span class="op">=</span> (<span class="vs">r&#39;sudo ssh-keygen -b 4096 -f /root/.ssh/id_rsa -N &quot;&quot; &amp;&amp; &#39;</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>              <span class="vs">r&#39;sudo cat /root/.ssh/id_rsa.pub&#39;</span> <span class="op">|</span> tee $HOME<span class="op">/</span>masterkey<span class="st">&#39;)</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="st">clusterconn.run_on_master_node(ssh_keygen)</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="st">clusterconn.copy_from_master_node(&#39;</span><span class="op">~/</span>masterkey<span class="st">&#39;, &#39;</span>masterkey<span class="st">&#39;)</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="st">clusterconn.copy_to_all_nodes(&#39;</span>masterkey<span class="st">&#39;, &#39;</span><span class="op">~/</span>masterkey<span class="st">&#39;)</span></span></code></pre></div>
<p>This approach directly generates the private key on the master node and avoids copying it around between different nodes to minimize potential security issues. The public key is then copied back from the master to all the other nodes. We will move the public keys to their final locations on the worker nodes in the next step.</p>
<h3 id="installing-the-beeond-software">Installing the BeeOND software</h3>
<p>BeeOND is available as prebuilt packages for the Ubuntu distribution that Azure ML uses. Installation is therefore a matter of adding the BeeOND package repository and then installing the BeeOND package. To do this efficiently using <code>ClusterConnector</code> we can create an install script to be run on all the nodes. We will also add some extra logic to this script to make sure that the SSH keys are copied to the correct place and set some configuration options for the BeeOND installer. Below is a script to provision BeeOND on systems running Debian 9/Ubuntu 18.04.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co">#!/bin/bash</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="bu">echo</span> <span class="st">&quot;Installing master public key:&quot;</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span> masterkey <span class="kw">|</span> <span class="fu">sudo</span> tee <span class="at">-a</span> /root/.ssh/authorized_keys</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="bu">echo</span> <span class="at">-e</span> <span class="st">&quot;\n\n### Provisioning BeeOND FS:\n&quot;</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="bu">echo</span> <span class="st">&quot;Adding BeeOND public key&quot;</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="fu">wget</span> <span class="at">-q</span> https://www.beegfs.io/release/latest-stable/gpg/DEB-GPG-KEY-beegfs <span class="at">-O-</span> <span class="kw">|</span> <span class="fu">sudo</span> apt-key add <span class="at">-</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="bu">echo</span> <span class="st">&quot;Adding BeeOND repo&quot;</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="fu">wget</span> <span class="at">-q</span> https://www.beegfs.io/release/beegfs_7.2/dists/beegfs-deb9.list <span class="at">-O-</span> <span class="kw">|</span> <span class="dt">\</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">sudo</span> tee /etc/apt/sources.list.d/beegfs-deb9.list <span class="op">&amp;&gt;</span>/dev/null</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a><span class="bu">echo</span> <span class="st">&quot;Updating apt index and installing BeeOND&quot;</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a><span class="fu">sudo</span> apt-get update <span class="at">-q</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a><span class="fu">sudo</span> apt-get install <span class="at">-y</span> beeond</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a><span class="bu">echo</span> <span class="st">&quot;Configuring path to kernel headers&quot;</span></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span> <span class="op">&lt;&lt;EOF</span> <span class="kw">|</span> <span class="fu">sudo</span> tee /etc/beegfs/beegfs-client-autobuild.conf</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a><span class="st">buildArgs=-j20 OFED_INCLUDE_PATH=/usr/src/ofa_kernel/default/include</span></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a><span class="st">buildEnabled=true</span></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a><span class="op">EOF</span></span></code></pre></div>
<p>This script does a number of different things. The first is to copy the public key we created in the previous step to it’s final location in the <code>/root</code> folder. Next the BeeOND file repository and it’s public key are added to the package manager configuration to allow us to install the BeeOND packages. Then the package manager index gets updated and the BeeOND software installed. Finally a small configuration snippet is added for the BeeOND client so that it can find some necessary build dependencies later.</p>
<p>Now we can use the <code>ClusterConnector</code> again to copy this script to all the nodes and run it (as root), this example assumes the script is saved as <a href="https://github.com/numericalalgorithmsgroup/AzureML_Best_Practice/blob/master/beeond/provisioning/provision_beeond.sh"><code>provision_beeond.sh</code></a>:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>clusterconn.copy_to_all_nodes(<span class="st">&#39;provision_beeond.sh&#39;</span>, <span class="st">&#39;~/provision_beeond.sh&#39;</span>)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>clusterconn.run_on_all_nodes(<span class="st">&#39;sudo bash ./provision_beeond.sh&#39;</span>)</span></code></pre></div>
<h3 id="starting-the-beeond-filesystem">Starting the BeeOND Filesystem</h3>
<p>Finally we can start the BeeOND filesystem. To do this we run the startup script on just one of the nodes, and provide it with the nodelist we uploaded earlier to tell BeeOND which nodes to use in the cluster. We also pass options to tell it what path to use for the underlying storage (<code>/mnt/resource</code> for the local NVMe on our VMs) and what path to mount the filesystem at (we will use <code>/mnt/scratch</code> for this tutorial). Once again we can use the clusterconnector for this:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>beecmd<span class="op">=</span><span class="st">&#39;sudo beeond start -n nodefile -f /root/bgconf -d /mnt/resource -c /mnt/scratch -F&#39;</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>clusterconn.run_on_master_node(beecmd)</span></code></pre></div>
<p>Along with the <code>ClusterConnector</code> class the examples accompanying this tutorial also include a <code>BeeONDClusterConnector</code> class. This implements all the steps in this section so that a ready-to-use cluster, complete with BeeOND filesystem can be created very simply:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> clusterconnector <span class="im">import</span> BeeONDClusterConnector</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>beeclusterconn <span class="op">=</span> BeeONDClusterConnector(</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>    workspace<span class="op">=</span>workspace,</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>    cluster_name<span class="op">=</span><span class="st">&quot;MyCluster&quot;</span>,</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>    ssh_key<span class="op">=</span>sshpubkey,</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>    vm_type<span class="op">=</span><span class="st">&quot;Standard_ND40rs_v2&quot;</span>,</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>    admin_user<span class="op">=</span><span class="st">&quot;clusteradmin&quot;</span>,</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>beeclusterconn.initialise(num_nodes<span class="op">=</span><span class="dv">4</span>, beeond_mnt<span class="op">=</span><span class="st">&quot;/mnt/scratch&quot;</span>)</span></code></pre></div>
<p>This implements all the steps shown above to create a fixed size 4 node cluster with the BeeOND filesystem mounted at <code>/mnt/scratch</code>.</p>
<h2 id="using-beeond-in-azureml-workloads">Using BeeOND in AzureML Workloads</h2>
<p>Like in our Mask-RCNN tutorial we will be using a custom Docker container to deploy our workload, and we define this in the <code>Environment</code> object for our training in the same way as before. The AzureML SDK provides various configuration options for Docker, including a way for us to pass custom Docker command line arguments. We can use this to pass an argument to bind mount our new BeeOND filesystem into the Docker container at whatever mount point we like. For example, to map our Beeond FS at <code>/mnt/scratch</code> to a directory <code>/data</code> in the container we would pass the argument <code>-v /mnt/scratch:/data</code>. Combining this with the code we used previously to create an environment based on a custom Dockerfile, gives:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> azureml.core <span class="im">import</span> Workspace, Environment</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>workspace <span class="op">=</span> Workspace.get(<span class="st">&quot;AzureMLDemo&quot;</span>)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>environment <span class="op">=</span> Environment(<span class="st">&quot;CosmoFlowDockerEnvironment&quot;</span>)</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>environment.docker.enabled <span class="op">=</span> <span class="va">True</span> <span class="co"># Tell AzureML we want to use Docker</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>environment.docker.base_dockerfile <span class="op">=</span> <span class="st">&quot;./MyDockerfile&quot;</span> <span class="co"># Path to local Dockerfile</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>environment.python.user_managed_dependencies <span class="op">=</span> <span class="va">True</span>  <span class="co"># AzureML shouldn&#39;t try to install things</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>environment.docker.arguments <span class="op">=</span> [<span class="st">&#39;-v&#39;</span>, <span class="st">&#39;/mnt/scratch:/data&#39;</span>] <span class="co"># Bind mount our BeeOND FS</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>environment <span class="op">=</span> environment.register(workspace) <span class="co"># Validate and register the environment</span></span></code></pre></div>
<p>Now when our workload runs it will be able to access the BeeOND filesystem under the <code>/data</code> directory inside the container.</p>
<h2 id="testing-the-performance">Testing the Performance</h2>
<p>To assess the performance of our filesystem we used the <a href="https://github.com/breuner/elbencho">elbencho</a> storage benchmarking tool. It has been specifically designed to test the performance of modern distributed storage filesystems and supports orchestration of operations from many clients simultaneously. elbencho supports a range of different tests including whole-device, filesystem read/write and GPU data transfer benchmarks. For this tutorial we opted to perform filesystem read/write tests to demonstrate the performance of BeeOND as a general purpose solution for high performance storage.</p>
<p>To actually run the benchmark we installed elbencho in a custom Docker container for use with AzureML. The <a href="https://github.com/numericalalgorithmsgroup/AzureML_Best_Practice/blob/master/beeond/Dockerfile"><code>Dockerfile</code></a> for this is available in the <a href="https://github.com/numericalalgorithmsgroup/AzureML_Best_Practice/tree/master/beeond">accompanying GitHub repository</a>. The benchmarking job can then be submitted similarly to in our <a href="https://github.com/numericalalgorithmsgroup/AzureML_Best_Practice/tree/master/maskrcnn">Mask R-CNN tutorial</a>. In this case we will instruct AzureML to run a bash script which orchestrates running elbencho with the desired configurations - we can do this by passing a <code>command</code> argument to <code>ScriptRunConfig</code> rather than a <code>script</code> argument, e.g.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>script_conf <span class="op">=</span> ScriptRunConfig(</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>        source_directory<span class="op">=</span><span class="st">&quot;scripts&quot;</span>,</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>        command<span class="op">=</span>[<span class="st">&quot;bash&quot;</span>, <span class="st">&quot;./run_elbencho_multifile.sh&quot;</span>, ...],</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>        compute_target<span class="op">=</span>clusterconnector.cluster,</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>        environment<span class="op">=</span>environment,</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>        distributed_job_config<span class="op">=</span>parallelconfig,</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>    )</span></code></pre></div>
<p>Note that <code>script</code> explicitly expects a Python script, while <code>command</code> will accept any valid Linux command line. The full job submission script can be found in the GitHub repository as <a href="https://github.com/numericalalgorithmsgroup/AzureML_Best_Practice/blob/master/beeond/beeond_run_elbencho.py"><code>beeond_run_elbencho.py</code></a>, along with the elbencho orchestration script <a href="https://github.com/numericalalgorithmsgroup/AzureML_Best_Practice/blob/master/beeond/scripts/run_elbencho_multifile.sh"><code>scripts/run_elbencho_multifile.sh</code></a>.</p>
<p>In our case we tested a 4 node BeeOND cluster of ND40rs_v2 instances. We benchmarked concurrent read and write performance from all nodes simultaneously, with filesizes from 256kB to 64MB and 1 to 128 I/O threads per client - recall that in this case cluster nodes are both a storage server and a client. Each test used a total of 12800 files and all file caches were dropped before and after each phase to ensure that read benchmarks are truly from NVMe and not from RAM cache. The results we got for our 4 node cluster is shown below.</p>
<figure>
<img src="./beeond_rw.png" alt="Beeond Filesystem Aggregate Read and Write Performance for 4 ND40rs_v2 Nodes" /><figcaption aria-hidden="true">Beeond Filesystem Aggregate Read and Write Performance for 4 ND40rs_v2 Nodes</figcaption>
</figure>
<p>These are impressive results for untuned “out of the box” performance on 4 nodes, with total system bandwidth reliably exceeding 10GB/s or 2.5GB/s/node for larger files with multiple parallel I/O threads per client. This performance is comfortably sufficient for CosmoFlow which needs around 2.5GB/s/node read performance for roughly 16MB data files and highly parallel access patterns. In some cases read performance can exceed 15 GB/s total system bandwidth and write performance can also reliably exceed 15 GB/s for parallel writes of larger files. Looking more closely at trends in the data we can also see that there are two distinct performance regimes visible here - metadata limited and bandwidth limited.</p>
<p>The metadata limited regime occurs for smaller files where the total throughput depends on the BeeGFS metadata server’s ability to keep up with the number of files being accessed. For example, at 256kB and 1MB files, for both read and write, performance increases when going from 1 to 8 parallel I/O threads per node, but as the number of threads increases further performance stalls and then degrades again as the metadata server becomes overloaded with requests. By default the BeeOND tool only sets up a single metadata server for a cluster instance, we could likely improve the small file <em>read</em> performance if we needed to by adding another metadata server. Improving write performance is a little more complicated with multiple metadata servers because synchronisation of the metadata becomes an issue.</p>
<p>At the other end of the spectrum is the I/O bandwidth limited case. This can be seen for large files where the rate of metadata requests is much lower and a lot more data needs to be transferred for each file. In this case, initially adding more I/O threads can improve performance, stalling once the I/O bandwidth of the NVMe disks is saturated, but there is a smaller performance dropoff caused by the increased contention than for the metadata limited case. This is more of an issue for reading than writing data, mostly due to the fact that when writing the system can additionally make use of write caching to improve effective write rates. This is not possible for the read benchmark as we deliberately drop all caches to ensure we are reading all data from NVMe. It is also important to note that throughput for any individual file is likely to be impacted by increased contention, slowing I/O for that file, although the effect is not visible when looking at the total system bandwidth.</p>
<h2 id="conclusion">Conclusion</h2>
<p>In this tutorial we have shown you how to set up a compute cluster in AzureML backed with a high performance BeeGFS on Demand (BeeOND) filesystem. You can use this to provide high performance storage for both containerised and non-containerised ML workflows on AzureML to maximise data throughput to the GPUs in your training cluster.</p>
<p>The accompanying Github repository includes all the scripts you need to set up a cluster and run the elbencho storage benchmarking tool as an example workload. Running this benchmark on our 4 node test cluster we were able to achieve total system bandwidth of over 15 GB/s read and write out of the box with no additional performance tuning.</p>
<p>In our <a href="#">next tutorial</a> we will demonstrate how to run the HPC-scale CosmoFlow deep learning model on a BeeOND enabled cluster and achieve supercomputer level performance in the Azure cloud.</p>
<p><strong>Copyright 2021, Numerical Algorithms Group Ltd (The)</strong></p>
<h4 id="contact">Contact:</h4>
<p>Numerical Algorithms Group Ltd<br> 30 St Giles’<br> Oxford<br> OX1 3LE<br> UK<br> <a href="https://www.nag.com">www.nag.com</a></p>
