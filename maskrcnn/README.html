<figure>
<img src="https://www.nag.com/themes/custom/nag/logo.png" alt="NAG Logo" /><figcaption aria-hidden="true">NAG Logo</figcaption>
</figure>
<h1 id="tutorial-training-at-scale-on-azureml">Tutorial: Training at Scale on AzureML</h1>
<p><em>Learn how to quickly train an AI model at scale using Azure Machine Learning.</em></p>
<p><strong>Note: The cloud is a fast-moving environment and things can change quickly. This tutorial is accurate as of March 2021 but you should check the <a href="https://docs.microsoft.com/en-gb/azure/machine-learning/">Azure Machine Learning Documentation</a> for the latest updates to the service.</strong></p>
<p>AI and Machine learning are transforming science, industry and business with an always expanding range of applications. The pace of progress is relentless and with models becoming ever more complex and datasets ever larger, a single GPU or even a single machine with multiple GPUs is often not enough. Distributed training on large GPU clusters is becoming a more common requirement. For many organizations, owning such a cluster is not the best solution, so the cloud is a natural way to access large GPU clusters. In this tutorial we will show you how to quickly train a distributed model on your own GPU cluster using Azure Machine Learning.</p>
<p>It’s no secret that cloud computing can be complex, especially when directly managing infrastructure such as VMs and virtual networks. However, by using appropriate managed services the underlying infrastructure management is handled by the cloud platform. The Azure Machine Learning service allows the user to manage all aspects of the training (or inference) being performed, while automatically managing the underlying infrastructure.</p>
<p>By the end of this tutorial, you will understand how to create an AzureML workspace and configure datasets, software environments and compute resources. You will also learn how to create and submit training jobs using the AzureML Python SDK.</p>
<p>We will use Mask R-CNN as an example of a large model which scales well to many GPUs on multiple nodes. Mask R-CNN is an image segmentation and object detection model which is designed to identify all objects in a provided image and create pixel-level masks for each detected object. The implementation we have chosen is available from the NVIDIA <a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Segmentation/MaskRCNN">Deep Learning Examples</a> repository on Github. The training dataset we will use is the COCO2017 (Common Objects in Context) dataset which is used for the MLPerf benchmark of Mask R-CNN. Below is an example of the output that is attainable from a well-trained Mask R-CNN implementation:</p>
<p><img src="https://user-images.githubusercontent.com/1381301/66535560-d3422200-eace-11e9-9123-5535d469db19.png" alt="Image Credit: Facebook AI Research (https://github.com/facebookresearch/detectron2/)" /> <a href="https://user-images.githubusercontent.com/1381301/66535560-d3422200-eace-11e9-9123-5535d469db19.png">Image Credit: Facebook AI Research (https://github.com/facebookresearch/detectron2/)</a></p>
<h3 id="prerequisites">Prerequisites</h3>
<p>To follow this tutorial you will need:</p>
<ul>
<li>An Azure subscription with ND40rs v2 instance quota for AzureML</li>
<li>Basic familiarity with the Linux Bash shell</li>
<li>Basic familiarity with the Python programming language</li>
</ul>
<h2 id="preparing-datasets-and-infrastructure">Preparing Datasets and Infrastructure</h2>
<p>Before we can do any training we must first create and configure the various Azure resources we will be using. First, we will create a storage account and upload the training dataset, then we can create an AzureML workspace and configure it to allow access to the dataset for training.</p>
<p>These first steps will be performed using the <a href="https://docs.microsoft.com/en-us/cli/azure/install-azure-cli">Azure CLI</a> tool and Bash shell. The Azure CLI can be installed on Windows, Linux and Mac OS, or is available via the <a href="https://docs.microsoft.com/en-us/azure/cloud-shell/quickstart">Azure cloud shell</a>. The Bash shell is available on Linux and Mac OS, or on Windows via the <a href="https://docs.microsoft.com/en-us/windows/wsl/install-win10%5D">Windows Subsystem for Linux</a> (WSL). Alternatively, all steps that require the Bash shell can also be achieved using an Azure VM instance running a Linux distribution (e.g Ubuntu 18.04).</p>
<p>Note: This tutorial assumes that you do not have any existing AzureML workspaces or other infrastructure provisioned and so goes through all the creation steps needed. However, if you have already created an AzureML workspace or suitable storage accounts you may wish to use them rather than create new ones.</p>
<h3 id="preparing-the-training-dataset">Preparing the Training Dataset</h3>
<p>AzureML supports accessing data from a variety of Azure storage options. However for large file-based datasets, such as the <a href="https://cocodataset.org">COCO2017</a> dataset we will be using in this example, the most appropriate storage backend is <a href="https://docs.microsoft.com/en-gb/azure/storage/blobs/storage-blobs-introduction">Azure blob storage</a>.</p>
<p>To use Azure blob as our storage backend we must first create a <a href="https://docs.microsoft.com/en-gb/azure/storage/common/storage-account-create">storage account</a> and <a href="https://docs.microsoft.com/en-gb/azure/storage/blobs/storage-blobs-introduction">blob container</a>. We can, of course, use any existing storage account for this, but for best performance, we recommended creating a new dedicated storage account for training datasets. This avoids sharing bandwidth and transaction caps with other workloads which could degrade training performance and increase compute costs. <em>It is also vital that the storage account is in the same region as the AzureML workspace. This ensures maximum performance as well as avoiding costs for data transfer between regions.</em></p>
<p>You can create a storage account and container via the Azure Portal or the Azure CLI. For this tutorial we will demonstrate using the Azure CLI:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> location=<span class="st">&quot;WestEurope&quot;</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> rg_name=<span class="st">&quot;MLRG&quot;</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> az group create <span class="at">--name</span> <span class="va">${rg_name}</span> <span class="at">--location</span> <span class="va">${location}</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> storage_name=mlstorage-<span class="va">$RANDOM</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> az storage account create <span class="at">--name</span> <span class="va">${storage_name}</span> <span class="at">--resource-group</span> <span class="va">${rg_name}</span> <span class="at">--location</span> <span class="va">${location}</span> <span class="at">--sku</span> Standard_LRS</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> container_name=coco2017</span></code></pre></div>
<p>In all the examples in this tutorial we create shell variables to hold names and other configuration options. These variables are then used in subsequent examples to refer to previously created resource. You should modify the names of resources and other options as appropriate to meet your needs.</p>
<p>Once you have created the storage account and container you should also obtain a <a href="https://docs.microsoft.com/en-us/azure/storage/common/storage-sas-overview">shared access token</a> (SAS) which can be used to grant AzureML access to the container. The following Azure CLI commands will create a temporary (1 month lifespan) SAS token with the appropriate permissions and stores it in the <code>${sas}</code> shell variable for future use in this tutorial:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> expiry=<span class="va">$(</span><span class="fu">date</span> <span class="at">-u</span> <span class="at">-d</span> <span class="st">&quot;1 month&quot;</span> <span class="st">&#39;+%Y-%m-%dT%H:%MZ&#39;</span><span class="va">)</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> sas=<span class="va">$(</span><span class="ex">az</span> storage account generate-sas <span class="at">--account-name</span> <span class="va">$storage_name</span> <span class="at">--expiry</span> <span class="va">$expiry</span> <span class="dt">\</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">--services</span> b <span class="at">--resource-types</span> co <span class="at">--permission</span> acdlpruw <span class="at">-o</span> tsv<span class="va">)</span></span></code></pre></div>
<p>Uploading the dataset can be done in various ways, including using the Azure Portal, <a href="https://azure.microsoft.com/en-us/features/storage-explorer/">Storage Explorer</a> or using the <a href="https://docs.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-v10"><code>azcopy</code></a> command-line tool. Using <code>azcopy</code> is the most flexible as it can be used with local files, Azure storage and various other cloud vendor storage technologies to manage upload to, download from and transfer between cloud storage accounts.</p>
<p>If you are following this tutorial using Mask RCNN you can download and prepare the COCO 2017 dataset with the following commands:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> mkdir cocotmp<span class="kw">;</span> <span class="bu">cd</span> cocotmp</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> wget http://images.cocodataset.org/zips/train2017.zip</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> unzip train2017.zip</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> unzip annotations_trainval2017.zip</span></code></pre></div>
<p>If you are downloading a dataset from the internet we recommend that you use an Azure Virtual Machine in the same region as the storage account to prepare the data. This avoids having to download and then re-upload the data over your local internet connection.</p>
<p>Once you have prepared your dataset it must be uploaded to Azure storage. To upload the prepared dataset to your chosen Azure storage container, make sure you are in the base directory for the dataset, then use the following <code>azcopy</code> command:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> azcopy copy <span class="at">-r</span> . https://<span class="va">${storage_name}</span>.blob.core.windows.net/<span class="va">${container_name}</span>/<span class="pp">?</span><span class="va">${sas}</span></span></code></pre></div>
<p>This command will recursively copy the contents of the current working directory and all subdirectories to the previously created Azure storage container.</p>
<h3 id="create-an-azureml-workspace">Create an AzureML Workspace</h3>
<p>The next step will be to create an AzureML workspace. This is a logical container or account that all of our machine learning configuration, data, experiments and outputs will be stored in. You can create an AzureML workspace either via the Azure portal, the Azure CLI or the AzureML Python SDK.</p>
<p>The workspace depends on various other Azure resources including a KeyVault, storage account, and container registry. By default new resources will be created, but you can optionally choose to use existing resources and grant the Workspace access to them. In this tutorial, we will not provide specific resources and so a new Keyvault, storage account and container registry will be automatically created along with the workspace.</p>
<p>To create a new workspace from the Azure CLI, use the following command:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> azml_workspace=<span class="st">&quot;MLWorkspace&quot;</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> az ml workspace create <span class="at">--resource-group</span> <span class="va">${rg_name}</span> <span class="dt">\</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">--workspace_name</span> <span class="va">${azml_workspace}</span> <span class="dt">\</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">--location</span> <span class="va">${location}</span></span></code></pre></div>
<p>This will typically take several minutes as all the dependent resources must also be created.</p>
<p>Once the workspace and it’s dependent resources are created we can begin configuring it with our datasets and training environment.</p>
<h3 id="registering-the-training-dataset">Registering the Training Dataset</h3>
<p>To make the training dataset available for our scripts to consume inside the AzureML training environment we need to register the dataset and the storage backend in AzureML. To do this the AzureML SDK provides <a href="https://docs.microsoft.com/en-us/azure/machine-learning/how-to-access-data"><code>Datastore</code></a> and <a href="https://docs.microsoft.com/en-us/azure/machine-learning/how-to-create-register-datasets"><code>Dataset</code></a> objects which define the storage backend configuration and dataset configuration respectively.</p>
<p>A <code>Datastore</code> object defines a connection to a specific storage backend and securely caches any required authentication information. In this case, we are connecting to the Azure Blob storage backend and authentication using a shared access signature (SAS). This can be done via the Machine Learning Studio, Azure CLI or the AzureML SDK.</p>
<p>To define a <code>Datastore</code> that attaches to our Azure Blob storage via the Azure CLI, use the following commmand:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> datastore_name=<span class="st">&quot;coco2017_blob&quot;</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> az ml datastore attach-blob <span class="dt">\</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">--resource-group</span> <span class="va">${rg_name}</span> <span class="dt">\</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">--workspace-name</span> <span class="va">${azml_workspace}</span> <span class="dt">\</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">--name</span> <span class="va">${datastore_name}</span> <span class="dt">\</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">--account-name</span> <span class="va">${storage_name}</span> <span class="dt">\</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">--container-name</span> <span class="va">${container_name}</span> <span class="dt">\</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">--sas-token</span> <span class="va">${sas}</span></span></code></pre></div>
<p>(Note: this assumes you have performed the previous steps to set the workspace name, storage account name, container name and shared access signature shell variables.)</p>
<p>We can now define a <code>DataSet</code> using this <code>Datastore</code>. Specifically, we will create a <code>FileDataSet</code> object, which defines a list of files from the storage which can be mounted or downloaded when we perform a training or inference run in the AzureML workspace. We can provide an explicit list of all the files in the dataset or provide a pattern using wildcards (‘*’). In this case, since the container contains only the dataset files and nothing else, we can use the pattern ‘/**’ to match all files and folders in the container recursively. Again, this can be done via the Machine Learning Studio, Azure CLI, or the AzureML SDK.</p>
<p>To define a <code>FileDataSet</code> for the COCO 2017 training set via the Azure CLI, first create a DataSet specification file containing the following:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode json"><code class="sourceCode json"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">&quot;datasetType&quot;</span><span class="fu">:</span> <span class="st">&quot;File&quot;</span><span class="fu">,</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>  <span class="dt">&quot;parameters&quot;</span><span class="fu">:</span> <span class="fu">{</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    <span class="dt">&quot;path&quot;</span><span class="fu">:</span> <span class="fu">{</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>        <span class="dt">&quot;datastoreName&quot;</span><span class="fu">:</span> <span class="st">&quot;coco2017_blob&quot;</span><span class="fu">,</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>        <span class="dt">&quot;relativePath&quot;</span><span class="fu">:</span> <span class="st">&quot;/**&quot;</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    <span class="fu">}</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">},</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>  <span class="dt">&quot;registration&quot;</span><span class="fu">:</span> <span class="fu">{</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    <span class="dt">&quot;description&quot;</span><span class="fu">:</span> <span class="st">&quot;COCO 2017 training and validation images&quot;</span><span class="fu">,</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>    <span class="dt">&quot;name&quot;</span><span class="fu">:</span> <span class="st">&quot;coco2017_trainval&quot;</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">},</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>  <span class="dt">&quot;schemaVersion&quot;</span><span class="fu">:</span> <span class="dv">1</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a><span class="fu">}</span></span></code></pre></div>
<p>If you are using a different dataset or naming scheme ensure you substitute the appropriate value of <code>dataStoreName</code> and <code>name</code>, then save the file in your working directory as e.g <code>dataset_spec.json</code>.</p>
<p>Now we can use the following command to register a <code>Dataset</code> object in our workspace with the spec file:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> dataset_name=<span class="st">&quot;coco2017_trainval&quot;</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> az ml dataset register <span class="at">--workspace-name</span> <span class="va">${azml_workspace}</span> <span class="dt">\</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">--resource-group</span> <span class="va">${rg_name}</span> <span class="dt">\</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">--file</span> dataset_spec.json</span></code></pre></div>
<p>This <code>Dataset</code> object can now be used in any AzureML jobs to access the dataset files we prepared and uploaded.</p>
<h2 id="submission-scripts">Submission Scripts</h2>
<p>The rest of this tutorial is concerned with configuring the compute environment and job configuration for individual training runs. This is typically unique to specific runs which may require a different software environment, number of compute nodes or type of compute node, as well as specific framework and model configuration.</p>
<p>We have provided example job submission scripts and their accompanying configuration files in the <a href="https://github.com/numericalalgorithmsgroup/AzureML_Best_Practice/tree/master/maskrcnn">AzureML_Best_Practice</a> Github repository. To obtain them you should download or clone this repository. The example Mask R-CNN training benchmark scripts can then be found in the <code>maskrcnn</code> subdirectory.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> git clone https://github.com/numericalalgorithmsgroup/AzureML_Best_Practice.git</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> cd maskrcnn</span></code></pre></div>
<p>Before running any of the provided scripts you should edit <code>sharedconfig.py</code> and provide the required information about the workspace and datasets configured in the previous section. This configuration file is then used by the submission scripts to connect to the correct workspace and submit jobs with the correct datasets.</p>
<p>We have provided two example submission and training scripts to demonstrate different potential usage scenarios. The first script (<a href="blobmount_submit_maskrcnn.py">blobmount_submit_maskrcnn.py</a>) performs training on datasets mounted over the network, the second (<a href="localdownload_submit_maskrcnn.py">localdownload_submit_maskrcnn.py</a>) explicitly downloads the dataset to the individual compute nodes before training. These scripts are designed to run Mask R-CNN in a benchmarking configuration. By default, the training will run for 1000 iterations with a variable batch size of 8 images per GPU. Optimal choices of other hyperparameters such as the learning rate and, warmup and decay schedules will vary depending on the batch size and number of GPUs and should be chosen accordingly.</p>
<p>To run either of these training scripts, first ensure you have installed the AzureML SDK as described at the top of this tutorial, then call the script specifying the desired number of nodes to train on. You can also select other options such as setting the number of iterations to train over (use <code>--help</code> to see all options). For example, to benchmark for 2000 iterations on 4 nodes:</p>
<pre><code>$ ./blobmount_submit_maskrcnn 4 --iter 2000</code></pre>
<p>The script will then configure a 4 cluster node in the AzureML environment and submit a 2000 iteration training job to it. The cluster is configured to automatically shut down idle nodes after 60 seconds to prevent incurring unnecessary costs after the job is completed.</p>
<h2 id="understanding-the-job-submission-scripts">Understanding the Job Submission Scripts</h2>
<p>In this final section of the tutorial, we will look at the steps that the submission script goes through to create a compute cluster, configure a training environment and submit a training job. The provided scripts automate these steps as follows:</p>
<h3 id="define-compute-environment">1. Define Compute Environment</h3>
<p>The <code>Environment</code> object collects configuration information about the desired state of the compute environment such as which Python packages should be installed or what Docker container to use. <code>Environment</code> objects are saved and versioned by the workspace to allow reuse and archive of all information required to reproduce any given training or inference job.</p>
<p>To define a custom environment via the AzureML Python SDK, we must create an <code>Environment</code> object, configure it as required and register it to the workspace. For example, to create an environment based on a Docker image defined in a local Dockerfile (named <code>./MyDockerfile</code>)</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> azureml.core <span class="im">import</span> Workspace, Environment</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>workspace <span class="op">=</span> Workspace.get(<span class="st">&quot;AzureMLDemo&quot;</span>)</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>environment <span class="op">=</span> Environment(<span class="st">&quot;CustomDockerEnvironment&quot;</span>)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>environment.docker.enabled <span class="op">=</span> <span class="va">True</span> <span class="co"># Tell AzureML we want to use Docker</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>environment.docker.base_dockerfile <span class="op">=</span> <span class="st">&quot;./MyDockerfile&quot;</span> <span class="co"># Path to local Dockerfile</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>environment.python.user_managed_dependencies <span class="op">=</span> <span class="va">True</span>  <span class="co"># AzureML shouldn&#39;t try to install things</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>environment <span class="op">=</span> environment.register(workspace) <span class="co"># Validate and register the environment</span></span></code></pre></div>
<p>This Environment object can now be passed to the configuration of a Run and AzureML will execute that run in the provided Docker container.</p>
<p>Passing a Dockerfile in this way will cause AzureML to build the Docker image in its attached Azure Container Registry instance, however, this is typically much slower than building the container on a dedicated host. AzureML also supports the use of prebuilt Docker images in both public and private container registries. The AzureML documentation on <a href="https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-custom-docker-image">using custom containers</a> shows how to use custom docker images hosted in another Azure Container registry or in a public registry.</p>
<h4 id="custom-docker-image">Custom Docker Image</h4>
<p>If you want to use a custom Docker image with AzureML it must provide the runtime dependencies needed by AzureML. Microsoft provides a repository of <a href="https://github.com/Azure/AzureML-Containers">example Docker containers for AzureML</a> on GitHub. The custom Docker image used for this tutorial is based on the NVIDIA NGC container image, with the following additions:</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode dockerfile"><code class="sourceCode dockerfile"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Build image on top of NVidia MXnet image</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="kw">ARG</span> FROM_IMAGE_NAME=nvcr.io/nvidia/pytorch:21.02-py3</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="kw">FROM</span> ${FROM_IMAGE_NAME}</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Pin Key Package Versions</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="kw">ENV</span> MOFED_VER 5.0-2.1.8.0</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="kw">ENV</span> AZML_SDK_VER 1.25.0</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Other required variables for MOFED drivers</span></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a><span class="kw">ENV</span> OS_VER ubuntu20.04</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a><span class="kw">ENV</span> PLATFORM x86_64</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a><span class="co">### Install Mellanox Drivers </span><span class="al">###</span></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a><span class="kw">RUN</span> apt-get update &amp;&amp; apt-get install -y libcap2 libfuse-dev &amp;&amp; \</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>    wget --quiet http://content.mellanox.com/ofed/MLNX_OFED-${MOFED_VER}/MLNX_OFED_LINUX-${MOFED_VER}-${OS_VER}-${PLATFORM}.tgz &amp;&amp; \</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>    tar -xvf MLNX_OFED_LINUX-${MOFED_VER}-${OS_VER}-${PLATFORM}.tgz &amp;&amp; \</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>    MLNX_OFED_LINUX-${MOFED_VER}-${OS_VER}-${PLATFORM}/mlnxofedinstall --user-space-only --without-fw-update --all --without-neohost-backend --force</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a><span class="co">### Install Python Dependencies </span><span class="al">###</span></span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a><span class="kw">RUN</span> pip install azureml-defaults==${AZML_SDK_VER}</span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a><span class="co">### Custom additions for specific training </span><span class="al">###</span></span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a><span class="co"># !!!! INSERT YOUR REQUIRED PACKAGE INSTALLATIONS HERE !!!!</span></span></code></pre></div>
<p>This provides all the core dependencies needed to run distributed training on AzureML with PyTorch. The NVIDIA Mask R-CNN reference implementation we are using for this tutorial is pre-installed into the base NGC container we are using. You should add your workload and any additional dependencies as appropriate. The complete <a href="Dockerfile">Dockerfile</a> is provided in this repository along with all the other tutorial code.</p>
<h3 id="define-cluster">Define cluster</h3>
<p>Finally, we must define our desired compute resources. AzureML supports both single compute instances and clusters, however, as this is a <em>distributed</em> Machine Learning tutorial we will only consider compute clusters. Compute clusters are persistent objects and support autoscaling up to a maximum number of nodes as well as down to zero nodes when the cluster is idle.</p>
<p>To create a compute cluster instance via the AzureML SDK we must first define a provisioning configuration for the compute cluster. Then we can create the cluster with this configuration:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> azureml.core <span class="im">import</span> Workspace</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> azureml.core.compute <span class="im">import</span> AmlCompute</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>workspace <span class="op">=</span> Workspace.get(<span class="st">&quot;AzureMLDemo&quot;</span>)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>cluster_config <span class="op">=</span> AmlCompute.provisioning_configuration(</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>  vm_size<span class="op">=</span><span class="st">&quot;Standard_ND40rs_v2&quot;</span>,</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>  min_nodes<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>  max_nodes<span class="op">=</span><span class="dv">16</span>,</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>  idle_seconds_before_scaledown<span class="op">=</span><span class="dv">300</span>,</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>  admin_username<span class="op">=</span><span class="st">&quot;clusteradmin&quot;</span>,</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>  admin_user_ssh_key<span class="op">=</span>sshpubkey, <span class="co"># Contents of a public key file</span></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>  remote_login_port_public_access<span class="op">=</span><span class="st">&quot;Enabled&quot;</span></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>cluster <span class="op">=</span> AmlCompute.create(workspace, <span class="st">&quot;MyCluster&quot;</span>, cluster_config)</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>cluster.wait_for_completion()</span></code></pre></div>
<p>This will create a cluster that can scale up to 16 ND40rs v2 nodes. The cluster will autoscale as needed to run jobs, and idle nodes will be shut down after 5 minutes. The last three options to the cluster configuration allow us to specify an admin username and ssh key and enable remote ssh access to the cluster. This can be useful for debugging or advanced usage that requires modifying the cluster manually.</p>
<h2 id="run-training">Run training</h2>
<p>Once we have defined datasets, a compute environment and a compute cluster, we can submit a training job. AzureML organises training jobs into groups or <code>Experiments</code>, each of which can then contain many runs.</p>
<p>For an existing model implementation such as Mask R-CNN, there will typically already be a training script. In this case, we can instruct AzureML to upload the script and any other supporting config files to the compute nodes and then run the script. This can only be done with the AzureML SDK by creating a <code>ScriptRunConfig</code> object. This config is then submitted to an experiment to begin a run.</p>
<p>The directory <code>train</code> in the submission script repository contains several example scripts for training Mask R-CNN. These have been adapted from the original training scripts from the NVIDIA repository to make them compatible with Azure.</p>
<p>Additionally, for distributed training on multiple nodes, we must provide information about this distributed configuration. For this, we provide an <code>MpiConfiguration</code> object as an argument to the <code>ScriptRunConfig</code>.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> azureml.core <span class="im">import</span> Workspace, Experiment, ScriptRunConfig, Dataset</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> azureml.core.runconfig <span class="im">import</span> MpiConfiguration</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> azureml.core.compute <span class="im">import</span> AmlCompute</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>workspace <span class="op">=</span> Workspace.get(<span class="st">&quot;AzureMLDemo&quot;</span>) <span class="co"># Get existing workspace</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>environment <span class="op">=</span> Environment.get(workspace, <span class="st">&quot;CustomDockerEnvironment&quot;</span>) <span class="co"># Get existing environment</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>cluster <span class="op">=</span> AmlCompute(workspace, <span class="st">&quot;MyCluster&quot;</span>) <span class="co"># Get existing cluster</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> Dataset.get_by_name(workspace, <span class="st">&quot;coco2017_trainval&quot;</span>)</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>experiment <span class="op">=</span> Experiment(workspace, <span class="st">&quot;MyExperiment&quot;</span>)</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>dist_config <span class="op">=</span> MpiConfiguration(node_count<span class="op">=</span><span class="dv">4</span>, process_count_per_node<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>jobconfig <span class="op">=</span> ScriptRunConfig(</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>  source_directory<span class="op">=</span><span class="st">&quot;train&quot;</span>,</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>  script<span class="op">=</span><span class="st">&quot;./train_net.py&quot;</span>,</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>  arguments<span class="op">=</span>[<span class="st">&quot;--dataset&quot;</span>, dataset.as_mount()],</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>  compute_target<span class="op">=</span>cluster,</span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>  environment<span class="op">=</span>environment,</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>  distributed_job_config<span class="op">=</span>dist_config</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>experiment.submit(jobconfig)</span></code></pre></div>
<p>This will upload the contents of the directory named <code>train</code><sup><a href="#CWD_note">1</a></sup> and run a training script named <code>train_net.py</code> on 4 nodes of the compute cluster with 8 processes per node (1 per GPU). We can instruct AzureML to mount the dataset on the Compute Cluster using the <code>as_mount()</code> method which returns the mounted location on the compute. This will be passed as an argument to the training script so that it can locate the training data.</p>
<p><a name="CWD_note"><sup>1</sup></a> The submission script should be run from the maskrcnn directory so that the <code>train</code> directory can be found and uploaded.</p>
<h3 id="save-result-data">Save result data</h3>
<p>There are multiple options for saving output data at the end of a run. The simplest is to place any outputs into a folder named <code>outputs</code> in the working directory of the training script. AzureML will automatically capture all files in the <code>outputs</code> folder and store them along with all the logging data from the run. These saved files can be downloaded later from the Machine Learning Studio run history blade. For example to capture final weights you could add code to your training script that saves the weights to a file <code>./outputs/final_weights.pkl</code>. Then when the run is complete the file <code>final_weights.pkl</code> will be available, along with all the other logs and outputs, via the AzureML Studio. More details can be found in the <a href="https://docs.microsoft.com/en-us/azure/machine-learning/how-to-save-write-experiment-files">AzureML documentation</a>.</p>
<p>Alternatively, you can include additional code to upload results files to a separate results dataset or directly to Azure storage.</p>
<h2 id="next-steps">Next steps</h2>
<p>If you have got to this point then you have successfully run a distributed training job, congratulations! Now you will most likely want to validate your new model. You can download your model weights and perform validation locally, or you can perform the validation using AzureML.</p>
<p>We were excited to test our new model straight away so we downloaded it and used inferencing demo scripts provided to segment a bike race photo of our own:</p>
<p><img src="./outputs.png" alt="Image Credit: Gordon Hatton" /> <a href="./outputs.png">Image Credit: Gordon Hatton</a></p>
<p>Once you are happy with the model it can be deployed for production inferencing. AzureML provides a range of <a href="https://docs.microsoft.com/en-gb/azure/machine-learning/concept-model-management-and-deployment">MLOps features</a> for managing model deployment including pipeline and lifecycle management, deployment for batch, real-time and edge inferencing scenarios.</p>
<h2 id="summary">Summary</h2>
<p>The Azure Machine Learning service allows fast deployment of ML workflows to the Azure cloud with support for large file-based datasets and distributed training at scale. This tutorial provides a complete demonstration of all the steps required to port the training of an existing Machine Learning Workflow (Mask R-CNN) to AzureML along with a large file-based dataset. Demonstration submission scripts for training runs are included in the <a href="https://github.com/numericalalgorithmsgroup/AzureML_Best_Practice/tree/master/maskrcnn">accompanying Github repository</a>.</p>
<h2 id="about-nag">About NAG</h2>
<p><a href="www.nag.com">NAG</a> has played a leading role in numerical, scientific and High Performance Computing (HPC) for over 50 years and is one of the few organizations that have genuine expertise and experience in all aspects of HPC and cloud. We offer a range of HPC and cloud services including:</p>
<ul>
<li><a href="https://www.nag.com/content/nag-cloud-hpc-migration-service">Cloud HPC Migration</a></li>
<li><a href="https://www.nag.com/content/software-modernization-service">Software Optimization</a></li>
<li><a href="https://www.nag.com/content/gpus-and-accelerator-code-tuning">Accelerator Porting and Tuning</a></li>
</ul>
<p>as well as fully bespoke consultancy in all aspects of HPC both on-premises and in the cloud. For more information please reach out to us at <a href="mailto:info@nag.com">info@nag.com</a>.</p>
