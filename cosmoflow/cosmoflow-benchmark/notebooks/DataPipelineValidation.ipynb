{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating the TFRecord data pipeline\n",
    "\n",
    "I have produced some tfrecord files. Now let's verify that they are correct, that they contain the same contents as the original HDF5 files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from functools import partial\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = '/project/projectdirs/m3363/www/cosmoUniverse_2019_05_4parE'\n",
    "output_dir = '/global/cscratch1/sd/sfarrell/cosmoflow-benchmark/data/cosmoUniverse_2019_05_4parE_tf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify all input HDF5 files\n",
    "all_input_files = sorted(glob.glob(os.path.join(input_dir, '*/*.hdf5')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/global/cscratch1/sd/sfarrell/cosmoflow-benchmark/data/cosmoUniverse_2019_05_4parE_tf/univ_ics_2019-03_a10177483_000.tfrecord',\n",
       " '/global/cscratch1/sd/sfarrell/cosmoflow-benchmark/data/cosmoUniverse_2019_05_4parE_tf/univ_ics_2019-03_a10177483_001.tfrecord',\n",
       " '/global/cscratch1/sd/sfarrell/cosmoflow-benchmark/data/cosmoUniverse_2019_05_4parE_tf/univ_ics_2019-03_a10177483_002.tfrecord',\n",
       " '/global/cscratch1/sd/sfarrell/cosmoflow-benchmark/data/cosmoUniverse_2019_05_4parE_tf/univ_ics_2019-03_a10177483_003.tfrecord']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose one input file\n",
    "h5_file = all_input_files[16]\n",
    "\n",
    "# Find the corresponding TFRecord files\n",
    "prefix = os.path.splitext(os.path.basename(h5_file))[0]\n",
    "tf_files = sorted(glob.glob(os.path.join(output_dir, prefix) + '*'))\n",
    "\n",
    "assert len(tf_files) == 64\n",
    "\n",
    "tf_files[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/global/cscratch1/sd/sfarrell/cosmoflow-benchmark/data/cosmoUniverse_2019_05_4parE_tf/univ_ics_2019-03_a10184840_000.tfrecord',\n",
       " '/global/cscratch1/sd/sfarrell/cosmoflow-benchmark/data/cosmoUniverse_2019_05_4parE_tf/univ_ics_2019-03_a10184840_001.tfrecord',\n",
       " '/global/cscratch1/sd/sfarrell/cosmoflow-benchmark/data/cosmoUniverse_2019_05_4parE_tf/univ_ics_2019-03_a10184840_002.tfrecord',\n",
       " '/global/cscratch1/sd/sfarrell/cosmoflow-benchmark/data/cosmoUniverse_2019_05_4parE_tf/univ_ics_2019-03_a10184840_003.tfrecord']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose one input file\n",
    "h5_file = all_input_files[18]\n",
    "\n",
    "# Find the corresponding TFRecord files\n",
    "prefix = os.path.splitext(os.path.basename(h5_file))[0]\n",
    "tf_files = sorted(glob.glob(os.path.join(output_dir, prefix) + '*'))\n",
    "\n",
    "assert len(tf_files) == 64\n",
    "\n",
    "tf_files[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['full', 'namePar', 'physPar', 'redshifts', 'unitPar']>\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(h5_file, mode='r') as f:\n",
    "    print(f.keys())\n",
    "    h5_x = f['full'][:]\n",
    "    h5_y = f['unitPar'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_universe(x, size):\n",
    "    n = x.shape[0] // size\n",
    "    # Loop over each split\n",
    "    for xi in np.split(x, n, axis=0):\n",
    "        for xij in np.split(xi, n, axis=1):\n",
    "            for xijk in np.split(xij, n, axis=2):\n",
    "                yield xijk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the TFRecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(sample_proto, shape=(128, 128, 128, 4)):\n",
    "    parsed_example = tf.io.parse_single_example(\n",
    "        sample_proto,\n",
    "        features = dict(x=tf.io.FixedLenFeature(shape, tf.float32),\n",
    "                        y=tf.io.FixedLenFeature([4], tf.float32))\n",
    "    )\n",
    "    # Decode the data and normalize\n",
    "    x, y = parsed_example['x'], parsed_example['y']\n",
    "    #x /= (tf.reduce_sum(x) / np.prod(shape))\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a dataset which loads all the TFRecords into one batch\n",
    "data = (tf.data.Dataset.from_tensor_slices(tf_files)\n",
    "        .apply(tf.data.TFRecordDataset)\n",
    "        .map(parse_data)\n",
    "        .batch(len(tf_files)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "next_batch = data.make_one_shot_iterator().get_next()\n",
    "with tf.Session() as sess:\n",
    "    tf_x, tf_y = sess.run(next_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in data:\n",
    "    tf_x, tf_y = x.numpy().astype(np.int16), y.numpy()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(536870912, 536870912)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare the sums\n",
    "h5_x.sum(), tf_x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 1, 1], dtype=int16), array([0, 0, 1, 1], dtype=int16))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h5_x[0,0,0], tf_x[0,0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loop over sub-volumes in HDF5 and check equality\n",
    "all([(h5_xijk == tf_xijk).all()\n",
    "     for h5_xijk, tf_xijk in zip(split_universe(h5_x, 128), tf_x)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-v1.15.0-cpu",
   "language": "python",
   "name": "tensorflow_intel_1.15.0_py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
